{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-1.3.0-py2.py3-none-any.whl (3.0 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-1.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/home/ec2-user/spark-2.4.5-bin-hadoop2.7/\")\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType, IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.feature import CountVectorizerModel, IDFModel, StandardScalerModel, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "import boto3\n",
    "import time\n",
    " \n",
    "# Wait for 5 seconds\n",
    "\n",
    "\n",
    "sqs = boto3.resource('sqs')\n",
    "\n",
    "APP_NAME = \"BigData\"\n",
    "conf = pyspark.SparkConf().setAll([('spark.app.name', APP_NAME)])\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlc = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ngram(payload_obj):\n",
    "    n=2\n",
    "    payload = str(payload_obj)\n",
    "    ngrams = ''\n",
    "    for i in range(0,len(payload)-n + 1):\n",
    "        ngrams += payload[i:i+n]+ ' '\n",
    "    return ngrams[:-1]\n",
    "\n",
    "\n",
    "# define a function to compute sentiments of the received tweets\n",
    "def get_prediction(queries):\n",
    "    try:\n",
    "        queries = queries.map(lambda w: Row(payload=w))\n",
    "        queries = sqlc.createDataFrame(queries)\n",
    "\n",
    "        queries = queries.withColumn('ngrams', ngrams(queries['payload']))\n",
    "        queries = tokenizer.transform(queries)\n",
    "        queries = vectorizer.transform(queries)\n",
    "        queries = idf_model.transform(queries)\n",
    "        queries = scalerModel.transform(queries)\n",
    "        preds = model.transform(queries)\n",
    "        preds.select('payload','prediction').show()\n",
    "\n",
    "    except : \n",
    "        print('No data')\n",
    "\n",
    "\n",
    "    \n",
    "ngrams = udf(to_ngram, StringType())\n",
    "tokenizer = Tokenizer.load('/home/ec2-user/notebooks/BigData-Http-injections/models/Tokenizer')\n",
    "vectorizer = CountVectorizerModel.load('/home/ec2-user/notebooks/BigData-Http-injections/models/Vectorizer')\n",
    "idf_model = IDFModel.load('/home/ec2-user/notebooks/BigData-Http-injections/models/idf')\n",
    "scalerModel = StandardScalerModel.load('/home/ec2-user/notebooks/BigData-Http-injections/models/scalerModel')\n",
    "model = LogisticRegressionModel.load('/home/ec2-user/notebooks/BigData-Http-injections/models/Logistic_Regression_Model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: hello!\n",
      "+-------+----------+\n",
      "|payload|prediction|\n",
      "+-------+----------+\n",
      "| hello!|       0.0|\n",
      "+-------+----------+\n",
      "\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n"
     ]
    }
   ],
   "source": [
    "def recieve():\n",
    "    queue = sqs.get_queue_by_name(QueueName='test')\n",
    "    while True:\n",
    "        messages = queue.receive_messages()\n",
    "        if len(messages) == 0:\n",
    "            print('No data')\n",
    "        else:\n",
    "            queries = []\n",
    "            for message in messages:\n",
    "                # Print out the body and author (if set)\n",
    "                print('query: {0}'.format(message.body))\n",
    "                # Let the queue know that the message is processed\n",
    "                queries += [message.body]\n",
    "                message.delete()\n",
    "            \n",
    "            queries = sc.parallelize(queries)\n",
    "            queries = queries.map(lambda w: Row(payload=w))\n",
    "            queries = sqlc.createDataFrame(queries)\n",
    "\n",
    "            queries = queries.withColumn('ngrams', ngrams(queries['payload']))\n",
    "            queries = tokenizer.transform(queries)\n",
    "            queries = vectorizer.transform(queries)\n",
    "            queries = idf_model.transform(queries)\n",
    "            queries = scalerModel.transform(queries)\n",
    "            preds = model.transform(queries)\n",
    "            preds.select('payload','prediction').show()\n",
    "        time.sleep(2)\n",
    "\n",
    "recieve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
