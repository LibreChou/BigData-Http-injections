{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requierments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading https://files.pythonhosted.org/packages/b1/c8/e6e1f6a303ae5122dc28d131b5a67c5eb87cbf8f7ac5b9f87764ea1b1e1e/findspark-1.3.0-py2.py3-none-any.whl\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-1.3.0\n",
      "Collecting confluent-kafka\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/2a/224ca99a63a83eafabfffca51d81e7798e8bf144f6540c7270bec14ca9ec/confluent_kafka-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (8.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 8.0MB 4.0MB/s ta 0:00:011   22% |███████                         | 1.8MB 3.5MB/s eta 0:00:02    33% |██████████▉                     | 2.7MB 5.1MB/s eta 0:00:02    36% |███████████▉                    | 3.0MB 4.8MB/s eta 0:00:02    87% |████████████████████████████    | 7.0MB 6.8MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: confluent-kafka\n",
      "Successfully installed confluent-kafka-1.4.0\n",
      "--2020-04-14 13:20:43--  https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8-assembly_2.11/2.4.0/spark-streaming-kafka-0-8-assembly_2.11-2.4.0.jar\n",
      "Resolving repo1.maven.org (repo1.maven.org)... 151.101.112.209\n",
      "Connecting to repo1.maven.org (repo1.maven.org)|151.101.112.209|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13385346 (13M) [application/java-archive]\n",
      "Saving to: ‘spark-streaming-kafka-0-8-assembly_2.11-2.4.0.jar’\n",
      "\n",
      "spark-streaming-kaf 100%[===================>]  12.76M  7.44MB/s    in 1.7s    \n",
      "\n",
      "2020-04-14 13:20:44 (7.44 MB/s) - ‘spark-streaming-kafka-0-8-assembly_2.11-2.4.0.jar’ saved [13385346/13385346]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark\n",
    "!pip install confluent-kafka\n",
    "# Downloaded from https://mvnrepository.com/artifact/org.apache.spark/spark-streaming-kafka-0-8-assembly_2.11\n",
    "# !wget https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8-assembly_2.11/2.4.0/spark-streaming-kafka-0-8-assembly_2.11-2.4.0.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "findspark.init('/usr/local/spark/spark-2.4.0-bin-hadoop2.7')\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars spark-streaming-kafka-0-8-assembly_2.11-2.4.0.jar pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.90.31:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>BigData</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=BigData>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "# from operator import add\n",
    "# import sys\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import pickle\n",
    "from pyspark.mllib.feature import HashingTF, IDF, StandardScaler\n",
    "from pyspark.mllib.classification import LogisticRegressionModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "APP_NAME = \"BigData\"\n",
    "sc = SparkContext(appName=APP_NAME)\n",
    "ssc = StreamingContext(sc, 2)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/hashedTF.pickle', 'rb') as f:\n",
    "    hashingTF = pickle.load(f)\n",
    "    \n",
    "htfVectors = sc.textFile(\"./data/htfVectors\").map(Vectors.parse)\n",
    "idf = IDF().fit(htfVectors)\n",
    "\n",
    "tfidf =  idf.transform(htfVectors)\n",
    "\n",
    "scaler = StandardScaler().fit(tfidf)\n",
    "\n",
    "model = LogisticRegressionModel.load(sc, \"./models/Logistic_Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"test\"\n",
    "brokers = \"localhost:9092\"\n",
    "kvs = KafkaUtils.createDirectStream(ssc, [topic], {\"metadata.broker.list\": brokers})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ngram(payload_obj):\n",
    "    n=2\n",
    "    payload = str(payload_obj)\n",
    "    ngrams = []\n",
    "    for i in range(0,len(payload)-n+1):\n",
    "        ngrams.append(payload[i:i+n])\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "def get_prediction(line):\n",
    "    try:\n",
    "        \n",
    "        predictions = model.predict(scaler.transform(idf.transform(hashingTF.transform(to_ngram(line.collect()[0])))))\n",
    "        print(line.collect() + predictions.collect())\n",
    "    except : \n",
    "        print('---------------No data----------------')\n",
    "\n",
    "def print_line(line):\n",
    "    print(line.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n",
      "---------------No data----------------\n"
     ]
    }
   ],
   "source": [
    "#lines = ssc.socketTextStream(\"localhost\", 9092)\n",
    "#ngrams = kvs.flatMap(lambda x : to_ngram(x))\n",
    "kvs.foreachRDD(get_prediction)\n",
    "#kvs.foreachRDD(to_ngram)\n",
    "ssc.start()\n",
    "ssc.awaitTermination()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KafkaUtils.createDirectStream?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
