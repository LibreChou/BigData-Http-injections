{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'findspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0e8a257f62be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/usr/local/spark/spark-2.4.0-bin-hadoop2.7/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstreaming\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStreamingContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'findspark'"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/local/spark/spark-2.4.0-bin-hadoop2.7/')\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType, IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.feature import CountVectorizerModel, IDFModel, StandardScalerModel, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "\n",
    "\n",
    "from urllib.parse import unquote\n",
    "\n",
    "def to_ngram(payload_obj):\n",
    "    n=2\n",
    "    payload = str(payload_obj)\n",
    "    ngrams = ''\n",
    "    for i in range(0,len(payload)-n + 1):\n",
    "        ngrams += payload[i:i+n]+ ' '\n",
    "    return ngrams[:-1]\n",
    "\n",
    "\n",
    "# define a function to compute sentiments of the received tweets\n",
    "def get_prediction(queries):\n",
    "    try:\n",
    "        queries = queries.map(lambda w: Row(payload=w))\n",
    "        queries = sqlc.createDataFrame(queries)\n",
    "\n",
    "        queries = queries.withColumn('ngrams', ngrams(queries['payload']))\n",
    "        queries = tokenizer.transform(queries)\n",
    "        queries = vectorizer.transform(queries)\n",
    "        queries = idf_model.transform(queries)\n",
    "        queries = scalerModel.transform(queries)\n",
    "        preds = model.transform(queries)\n",
    "        preds.select('payload','prediction').show()\n",
    "\n",
    "    except : \n",
    "        print('No data')\n",
    "\n",
    "APP_NAME = \"BigData\"\n",
    "conf = pyspark.SparkConf().setAll([ ('spark.app.name', APP_NAME),('spark.executor.memory', '8g'), ('spark.cores.max', '2'), ('spark.driver.memory','8g')])\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlc = SQLContext(sc)\n",
    "    \n",
    "ngrams = udf(to_ngram, StringType())\n",
    "tokenizer = Tokenizer.load('models/Tokenizer')\n",
    "vectorizer = CountVectorizerModel.load('models/Vectorizer')\n",
    "idf_model = IDFModel.load('models/idf')\n",
    "scalerModel = StandardScalerModel.load('models/scalerModel')\n",
    "model = LogisticRegressionModel.load('models/Logistic_Regression_Model')\n",
    "ssc = StreamingContext(sc, batchDuration= 3)\n",
    "lines = ssc.textFileStream(\"data/mixedShuffleData.txt\")\n",
    "\n",
    "lines.foreachRDD(get_prediction)\n",
    "\n",
    "ssc.start()             \n",
    "ssc.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
